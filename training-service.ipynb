{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "71fe81dc"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import absl.logging\n",
        "\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "# 0 = all messages are logged (default behavior)\n",
        "# 1 = INFO messages are not printed\n",
        "# 2 = INFO and WARNING messages are not printed\n",
        "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
        "\n",
        "# On Mac you may encounter an error related to OMP, this is a workaround, but slows down the code\n",
        "# https://github.com/dmlc/xgboost/issues/1715\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\""
      ],
      "id": "71fe81dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "132953ea"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "tf.__version__"
      ],
      "id": "132953ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ca9a91"
      },
      "outputs": [],
      "source": [
        "if tf.test.gpu_device_name():\n",
        "    print(\"Default GPU Device:{}\".format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF if you have one.\")"
      ],
      "id": "43ca9a91"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gitpython\n",
        "\n",
        "import git\n",
        "repo_url = 'https://github.com/isl-org/OpenBot.git'  # Replace with your GitHub repository URL\n",
        "repo_destination = '/content/openbot'  # Destination folder in Colab\n",
        "\n",
        "# Function to check if a folder is empty\n",
        "def is_folder_empty(folder_path):\n",
        "    return not os.path.exists(folder_path) or not os.listdir(folder_path)\n",
        "\n",
        "\n",
        "if is_folder_empty(repo_destination):\n",
        "  git.Repo.clone_from(repo_url, repo_destination)\n",
        "  cloned_contents = os.listdir(repo_destination)\n",
        "  print(\"Cloned Repository Contents:\", cloned_contents)\n",
        "else:\n",
        "    print(\"Destination folder is not empty. Skipping cloning.\")"
      ],
      "metadata": {
        "id": "ldN6mtQ1Sjo-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ldN6mtQ1Sjo-"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2ac37fbe"
      },
      "outputs": [],
      "source": [
        "from openbot.policy.openbot import dataloader, data_augmentation, utils, train"
      ],
      "id": "2ac37fbe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c28641"
      },
      "source": [
        "## Set train and test dirs"
      ],
      "id": "05c28641"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc941e8e"
      },
      "source": [
        "Define the dataset directory and give it a name. Inside the dataset folder, there should be two folders, `train_data` and `test_data`."
      ],
      "id": "bc941e8e"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "823ef8f3"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"dataset\"\n",
        "dataset_name = \"openbot\"\n",
        "train_data_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "test_data_dir = os.path.join(dataset_dir, \"test_data\")"
      ],
      "id": "823ef8f3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add train and test data"
      ],
      "metadata": {
        "id": "qOsUBQDMU60i"
      },
      "id": "qOsUBQDMU60i"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "UPLOAD_FOLDER_TRAIN = '/content/openbot/policy/dataset/train_data/my_dataset_1'\n",
        "UPLOAD_FOLDER_TEST = '/content/openbot/policy/dataset/test_data/my_dataset_2'\n",
        "ALLOWED_EXTENSIONS = {'zip'}\n",
        "UPLOAD_FOLDER_TRAIN_TEXT = \"Upload train zip file\"\n",
        "UPLOAD_FOLDER_TEST_TEXT = \"Upload test zip file\"\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "def upload_and_extract(upload_folder,upload_folder_text):\n",
        "\n",
        "    print(upload_folder_text)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if len(uploaded) == 0:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "\n",
        "    if allowed_file(filename):\n",
        "        zip_folder = os.path.splitext(filename)[0]\n",
        "        zip_folder_path = os.path.join(upload_folder, zip_folder)\n",
        "        os.makedirs(zip_folder_path, exist_ok=True)\n",
        "\n",
        "        zip_filepath = os.path.join(zip_folder_path, filename)\n",
        "        with open(zip_filepath, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "\n",
        "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "            zip_ref.extractall(zip_folder_path)\n",
        "\n",
        "        os.remove(zip_filepath)\n",
        "\n",
        "        print(f\"File '{filename}' successfully uploaded and extracted to '{zip_folder_path}'.\")\n",
        "    else:\n",
        "        print(\"Invalid file format. Please upload a valid ZIP file.\")\n",
        "\n",
        "upload_and_extract(UPLOAD_FOLDER_TRAIN,UPLOAD_FOLDER_TRAIN_TEXT)\n",
        "upload_and_extract(UPLOAD_FOLDER_TEST,UPLOAD_FOLDER_TEST_TEXT)\n"
      ],
      "metadata": {
        "id": "3P7hOynsEoU8"
      },
      "id": "3P7hOynsEoU8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n"
      ],
      "metadata": {
        "id": "h6gDqgR0Uxf7"
      },
      "id": "h6gDqgR0Uxf7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "625eb2bd"
      },
      "source": [
        "You may have to tune the learning rate and batch size depending on your available compute resources and dataset. As a general rule of thumb, if you increase the batch size by a factor of n, you can increase the learning rate by a factor of sqrt(n). In order to accelerate training and make it more smooth, you should increase the batch size as much as possible. In our paper we used a batch size of 128. For debugging and hyperparamter tuning, you can set the number of epochs to a small value like 10. If you want to train a model which will achieve good performance, you should set it to 50 or more. In our paper we used 100."
      ],
      "id": "625eb2bd"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "a14c7cac"
      },
      "outputs": [],
      "source": [
        "params = train.Hyperparameters()\n",
        "\n",
        "params.MODEL = \"pilot_net\"  # choices: \"pilot_net\",\"cil_mobile\",\"cil_mobile_fast\",\"cil\"\n",
        "params.POLICY = \"autopilot\"  # choices: \"autopilot\",\"point_goal_nav\"\n",
        "params.TRAIN_BATCH_SIZE = 128\n",
        "params.TEST_BATCH_SIZE = 16\n",
        "params.LEARNING_RATE = 0.0003\n",
        "params.NUM_EPOCHS = 100\n",
        "params.BATCH_NORM = True  # use batch norm (recommended)\n",
        "params.FLIP_AUG = False  # flip image and controls as augmentation (only autopilot)\n",
        "params.CMD_AUG = False  # randomize high-level command as augmentation (only autopilot)\n",
        "params.USE_LAST = False  # resume training from last checkpoint\n",
        "params.WANDB = False\n",
        "# policy = \"autopilot\": images are expected to be 256x96 - no cropping required\n",
        "# policy = \"point_goal_nav\": images are expected to be 160x120 - cropping to 160x90\n",
        "params.IS_CROP = params.POLICY == \"point_goal_nav\""
      ],
      "id": "a14c7cac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ac0929"
      },
      "source": [
        "## Pre-process the dataset"
      ],
      "id": "82ac0929"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4b4494b4"
      },
      "outputs": [],
      "source": [
        "tr = train.Training(params)\n",
        "tr.dataset_name = dataset_name\n",
        "tr.train_data_dir = os.path.join(\"/content\",\"openbot\", \"policy\", \"dataset\", \"train_data\")\n",
        "tr.test_data_dir = os.path.join(\"/content\",\"openbot\", \"policy\", \"dataset\", \"test_data\")"
      ],
      "id": "4b4494b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abfc9b9b"
      },
      "source": [
        "Running this for the first time will take some time. This code will match image frames to the controls (labels) and indicator signals (commands).  By default, data samples where the vehicle was stationary will be removed. If this is not desired, you need to set `tr.remove_zeros = False`. If you have made any changes to the sensor files, changed `remove_zeros` or moved your dataset to a new directory, you need to set `tr.redo_matching = True`."
      ],
      "id": "abfc9b9b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9d78141"
      },
      "outputs": [],
      "source": [
        "tr.redo_matching = False\n",
        "tr.remove_zeros = True\n",
        "train.process_data(tr)"
      ],
      "id": "e9d78141"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "c531aecf"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "\n",
        "def broadcast(event, payload=None):\n",
        "    print(event, payload)\n",
        "\n",
        "\n",
        "event = threading.Event()\n",
        "my_callback = train.MyCallback(broadcast, event)"
      ],
      "id": "c531aecf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fc670c9"
      },
      "source": [
        "In the next step, you can convert your dataset to a tfrecord, a data format optimized for training. You can skip this step if you already created a tfrecord before or if you want to train using the files directly."
      ],
      "id": "2fc670c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59336936"
      },
      "outputs": [],
      "source": [
        "train.create_tfrecord(my_callback, policy=tr.hyperparameters.POLICY)"
      ],
      "id": "59336936"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a4a1cf"
      },
      "source": [
        "## Load the dataset"
      ],
      "id": "b0a4a1cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a51ee383"
      },
      "source": [
        "If you did not create a tfrecord and want to load and buffer files from disk directly, set `no_tf_record = True`."
      ],
      "id": "a51ee383"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bc2d2068"
      },
      "outputs": [],
      "source": [
        "no_tf_record = False"
      ],
      "id": "bc2d2068"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "082c90bf"
      },
      "outputs": [],
      "source": [
        "if no_tf_record:\n",
        "    tr.train_data_dir = train_data_dir\n",
        "    tr.test_data_dir = test_data_dir\n",
        "    train.load_data(tr, verbose=0)\n",
        "else:\n",
        "    tr.train_data_dir = os.path.join(dataset_dir, \"/content/openbot/policy/dataset/tfrecords/train.tfrec\")\n",
        "    tr.test_data_dir = os.path.join(dataset_dir, \"/content/openbot/policy/dataset/tfrecords/test.tfrec\")\n",
        "    train.load_tfrecord(tr, verbose=0)\n",
        "\n",
        "models_dir = \"/content/openbot/policy/models\"\n",
        "model_dir = os.path.join(models_dir, tr.model_name)\n",
        "os.makedirs(model_dir, exist_ok=True)  # Create model directory if it doesn't exist"
      ],
      "id": "082c90bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edf34f3f"
      },
      "outputs": [],
      "source": [
        "# Select interactive backend to show inline\n",
        "%matplotlib inline\n",
        "utils.show_batch(dataset=tr.train_ds, policy=tr.hyperparameters.POLICY, model=None)"
      ],
      "id": "edf34f3f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a5d0f77"
      },
      "source": [
        "## Training"
      ],
      "id": "0a5d0f77"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71034420"
      },
      "source": [
        "The number of epochs is proportional to the training time. One epoch means going through the complete dataset once. Increasing `NUM_EPOCHS` will mean longer training time, but generally leads to better performance. To get familiar with the code it can be set to small values like `5` or `10`. To train a model that performs well, it should be set to values between `50` and `200`. Setting `USE_LAST` to `true` will resume the training from the last checkpoint. The default values are `NUM_EPOCHS = 100` and `USE_LAST = False`. They are set in [Hyperparameters](#hyperparameters)."
      ],
      "id": "71034420"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "894cd54f"
      },
      "outputs": [],
      "source": [
        "# params.NUM_EPOCHS = 10\n",
        "# params.USE_LAST = True\n",
        "\n",
        "train.do_training(tr, my_callback, verbose=1)"
      ],
      "id": "894cd54f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cfd4aac"
      },
      "source": [
        "## Evaluation"
      ],
      "id": "0cfd4aac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f623b65f"
      },
      "source": [
        "The loss and mean absolute error should decrease. This indicates that the model is fitting the data well. The custom metrics (direction and angle) should go towards 1. These provide some additional insight to the training progress. The direction metric measures weather or not predictions are in the same direction as the labels. Similarly the angle metric measures if the prediction is within a small angle of the labels. The intuition is that driving in the right direction with the correct steering angle is most critical part for good final performance."
      ],
      "id": "f623b65f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4867aaa7"
      },
      "source": [
        "### Plot metrics"
      ],
      "id": "4867aaa7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ca0b291"
      },
      "outputs": [],
      "source": [
        "x = np.arange(tr.INITIAL_EPOCH + 1, tr.history.params[\"epochs\"] + 1, 1)"
      ],
      "id": "8ca0b291"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efd65e1a"
      },
      "outputs": [],
      "source": [
        "plt.figure().gca().xaxis.get_major_locator().set_params(integer=True)\n",
        "plt.plot(x, tr.history.history[\"loss\"], label=\"loss\")\n",
        "plt.plot(x, tr.history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(os.path.join(tr.log_path, \"loss.png\"), bbox_inches=\"tight\")"
      ],
      "id": "efd65e1a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97e98f63"
      },
      "outputs": [],
      "source": [
        "plt.figure().gca().xaxis.get_major_locator().set_params(integer=True)\n",
        "plt.plot(x, tr.history.history[\"mean_absolute_error\"], label=\"mean_absolute_error\")\n",
        "plt.plot(\n",
        "    x, tr.history.history[\"val_mean_absolute_error\"], label=\"val_mean_absolute_error\"\n",
        ")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Absolute Error\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(os.path.join(tr.log_path, \"error.png\"), bbox_inches=\"tight\")"
      ],
      "id": "97e98f63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa1752d7"
      },
      "outputs": [],
      "source": [
        "plt.figure().gca().xaxis.get_major_locator().set_params(integer=True)\n",
        "plt.plot(x, tr.history.history[\"direction_metric\"], label=\"direction_metric\")\n",
        "plt.plot(x, tr.history.history[\"val_direction_metric\"], label=\"val_direction_metric\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Direction Metric\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(os.path.join(tr.log_path, \"direction.png\"), bbox_inches=\"tight\")"
      ],
      "id": "aa1752d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9de04eec"
      },
      "outputs": [],
      "source": [
        "plt.figure().gca().xaxis.get_major_locator().set_params(integer=True)\n",
        "plt.plot(x, tr.history.history[\"angle_metric\"], label=\"angle_metric\")\n",
        "plt.plot(x, tr.history.history[\"val_angle_metric\"], label=\"val_angle_metric\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Angle Metric\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(os.path.join(tr.log_path, \"angle.png\"), bbox_inches=\"tight\")"
      ],
      "id": "9de04eec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e72c1de3"
      },
      "source": [
        "### Save tf lite models for best train, best val and last checkpoint"
      ],
      "id": "e72c1de3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92a1ef62"
      },
      "outputs": [],
      "source": [
        "best_train_checkpoint = \"cp-best-train.ckpt\"\n",
        "best_train_tflite = utils.generate_tflite(tr.checkpoint_path, best_train_checkpoint)\n",
        "utils.save_tflite(best_train_tflite, tr.checkpoint_path, \"best-train\")\n",
        "best_train_index = np.argmin(np.array(tr.history.history[\"loss\"]))\n",
        "print(\n",
        "    \"Best Train Checkpoint (epoch %s) - angle: %.4f, val_angle: %.4f, direction: %.4f, val_direction: %.4f\"\n",
        "    % (\n",
        "        best_train_index,\n",
        "        tr.history.history[\"angle_metric\"][best_train_index],\n",
        "        tr.history.history[\"val_angle_metric\"][best_train_index],\n",
        "        tr.history.history[\"direction_metric\"][best_train_index],\n",
        "        tr.history.history[\"val_direction_metric\"][best_train_index],\n",
        "    )\n",
        ")"
      ],
      "id": "92a1ef62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb605b90"
      },
      "outputs": [],
      "source": [
        "best_val_checkpoint = \"cp-best-val.ckpt\"\n",
        "best_val_tflite = utils.generate_tflite(tr.checkpoint_path, best_val_checkpoint)\n",
        "utils.save_tflite(best_val_tflite, tr.checkpoint_path, \"best\")\n",
        "utils.save_tflite(best_val_tflite, tr.checkpoint_path, \"best-val\")\n",
        "best_val_index = np.argmin(np.array(tr.history.history[\"val_loss\"]))\n",
        "print(\n",
        "    \"Best Val Checkpoint (epoch %s) - angle: %.4f, val_angle: %.4f, direction: %.4f, val_direction: %.4f\"\n",
        "    % (\n",
        "        best_val_index,\n",
        "        tr.history.history[\"angle_metric\"][best_val_index],\n",
        "        tr.history.history[\"val_angle_metric\"][best_val_index],\n",
        "        tr.history.history[\"direction_metric\"][best_val_index],\n",
        "        tr.history.history[\"val_direction_metric\"][best_val_index],\n",
        "    )\n",
        ")"
      ],
      "id": "bb605b90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e2b1644"
      },
      "outputs": [],
      "source": [
        "last_checkpoint = \"cp-last.ckpt\"\n",
        "last_tflite = utils.generate_tflite(tr.checkpoint_path, last_checkpoint)\n",
        "utils.save_tflite(last_tflite, tr.checkpoint_path, \"last\")\n",
        "print(\n",
        "    \"Last Checkpoint - angle: %.4f, val_angle: %.4f, direction: %.4f, val_direction: %.4f\"\n",
        "    % (\n",
        "        tr.history.history[\"angle_metric\"][-1],\n",
        "        tr.history.history[\"val_angle_metric\"][-1],\n",
        "        tr.history.history[\"direction_metric\"][-1],\n",
        "        tr.history.history[\"val_direction_metric\"][-1],\n",
        "    )\n",
        ")"
      ],
      "id": "0e2b1644"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c57018"
      },
      "source": [
        "### Evaluate the best model (train loss) on the training set"
      ],
      "id": "d0c57018"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04a03c6d"
      },
      "outputs": [],
      "source": [
        "best_train_model = utils.load_model(\n",
        "    os.path.join(tr.checkpoint_path, best_train_checkpoint),\n",
        "    tr.loss_fn,\n",
        "    tr.metric_list,\n",
        "    tr.custom_objects,\n",
        ")\n",
        "loss, mae, direction, angle = best_train_model.evaluate(\n",
        "    tr.train_ds,\n",
        "    steps=tr.image_count_train / tr.hyperparameters.TRAIN_BATCH_SIZE,\n",
        "    verbose=1,\n",
        ")"
      ],
      "id": "04a03c6d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92462709"
      },
      "outputs": [],
      "source": [
        "utils.show_batch(\n",
        "    dataset=tr.train_ds, policy=tr.hyperparameters.POLICY, model=best_train_model\n",
        ")"
      ],
      "id": "92462709"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10a078b1"
      },
      "outputs": [],
      "source": [
        "utils.compare_tf_tflite(best_train_model, best_train_tflite)"
      ],
      "id": "10a078b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ad2605"
      },
      "source": [
        "### Evaluate the best model (val loss) on the validation set"
      ],
      "id": "b2ad2605"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2ac5b20"
      },
      "outputs": [],
      "source": [
        "best_val_model = utils.load_model(\n",
        "    os.path.join(tr.checkpoint_path, best_val_checkpoint),\n",
        "    tr.loss_fn,\n",
        "    tr.metric_list,\n",
        "    tr.custom_objects,\n",
        ")\n",
        "loss, mae, direction, angle = best_val_model.evaluate(\n",
        "    tr.test_ds,\n",
        "    steps=tr.image_count_test / tr.hyperparameters.TEST_BATCH_SIZE,\n",
        "    verbose=1,\n",
        ")"
      ],
      "id": "e2ac5b20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8e01e77"
      },
      "outputs": [],
      "source": [
        "utils.show_batch(\n",
        "    dataset=tr.test_ds, policy=tr.hyperparameters.POLICY, model=best_val_model\n",
        ")"
      ],
      "id": "d8e01e77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daf9dbac"
      },
      "outputs": [],
      "source": [
        "utils.compare_tf_tflite(best_val_model, best_val_tflite)"
      ],
      "id": "daf9dbac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29c9d606"
      },
      "source": [
        "## Save the notebook as HTML"
      ],
      "id": "29c9d606"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c47915f"
      },
      "outputs": [],
      "source": [
        "utils.save_notebook()\n",
        "current_file = \"policy_learning.ipynb\"\n",
        "output_file = os.path.join(tr.log_path, \"notebook.html\")\n",
        "utils.output_HTML(current_file, output_file)"
      ],
      "id": "3c47915f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa902ebd"
      },
      "outputs": [],
      "source": [],
      "id": "fa902ebd"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}